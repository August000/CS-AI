Answer questions marked as "QS"

QS1.1: What data structure do you use for implementing DFS? Why?
Answer: In DFS serch alogoithm you use a stack so that you can add elements to the fringe and iterate through them using a while loop until the fringe is empty

QS1.2: The Pacman board will show an overlay of the states explored, and the order in which they were explored (brighter red means earlier exploration). Is the exploration order what you would have expected? Does Pacman actually go to all the explored squares on his way to the goal?
Answer: Yes, the exploration consist of subnodes that were explored earlier by the algorithm, however, they might lead of a dead end that leads to the exploration of another nodes. The behavior of the pacman is expected to go directly to the goal since we are not returning the nodes itself for the pacman, we are returing the first result of actions that lead to the goal and not all the explore nodes. 

QS2.1:


QS3.1:  What cost function did you use?  Please briefly describe how your cost function is designed (what factors does it consider and how heavy of a weight has each factor considered been given?)
The cost function used when running the "python pacman.py -l mediumMaze -p SearchAgent -a fn=ucs" will be the default defined by SearchAgent. 
This is defined by "costFn = lambda x: 1", and this means that for every move the cost is one no matter what. 
By contrast, StayEastSearchAgent defines a cost function of "costFn = lambda pos: 0.5 ** pos[0]", meaning that it considers the x position value when deciding the cost. 
In this case, the cost of a move is 0.5^x_pos, so moves that are on the east side of the board(positive x direction) are much cheaper.
In the case of StayWestSearchAgent, which defines "costFn = lambda pos: 2 ** pos[0]", the cost of a move is 2^x_pos.
This means that in this case, moves on the east side of the board(positive x direction) are much more expensive.

QS4.1: What is the difference between the nullHeuristic  and the Manhattan distance heuristic? Does nullHeuristic give the optimal solution?  Why is the Manhattan distance heuristic better?
The nullHeuristic will always return 0, so it isn't giving us any extra info past what we were using for UCS. Thus, the algorithim will still find the optimal solution using nullHeuristic but will behave just like UCS would.
By contrast, the Manhattan distance heuristic will actually provide information on how close the node is to the goal state, which will allow it to more optimally choose nodes to expand.
This helps us explain why the Manhattan distance heuristic is better than the nullHeuristic. They will both find the optimal solution, but the Manhattan distance heuristic will find it while expanding less nodes.

QS4.2: What happens on openMaze for the various search strategies?
DFS finds a very long path that goes back and forth row by row until it finds the goal. 
BFS, UCS, and A*(with Manhattan) all find the same optimal path which goes left for a period until it's past a blocker, then goes down to the bottom and continues left until it reaches the goal in the bottom left side. 
However, while BFS and UCS both expand 682 nodes, A* is able to find the path while only expanding 535 nodes.


QS5.1: What states representation do you choose in this problem?
I chose ((x,y), (a,b,c,d)) as a state representation, where (x,y) denotes the position of the agent and (a,b,c,d) is a tuple of booleans denoting if a given corner has been visited.


QS5.2: Briefly explain your implementation
getStartState gets the starting position and initializes a list of Boolean values indicating if each of the 4 corners has been visited.  
Then it checks if the start state is in a corner and marks it as visited if so. Then it turns it into a tuple and joins it with the position to return the start state (start, visited_corners). 

isGoalState takes the current state and returns true if all 4 of the corners have been visited(all 4 boolean values in visited_corners are true), returns false otherwise.

getSuccessors loops through each of the directions that the agent could travel. 
If a move in the given direction doesn't lead into a wall, it updates the visited_corners list(if the next node is a corner) and then adds the next state to the successor list with a cost of 1.
After going through all potential moves, the successor list is returned.


QS6.1:


QS7.1: What heuristic did you use for this problem?  Explain the potential strong points and weak points of your chosen heuristic.  Is your heuristic consistent?  Why?
Our UCS agent finds the optimal solution in about 13 seconds, exploring over 16,000 nodes.

The heuristic we used simply calculated the Manhattan distance between pacman's position and the farthest food dot. 
This works well because we know that the agent will at the very least have to travel as far as the farthest dot away from itself. 
If there are other dots which are out of the way, that can only make the cost greater and thus our algorithm cannot overestimate the cost. 
This means that our algorithm is admissible. Furthermore, we also know it's consistent because the heuristic can change by at-most 1 for 1 step.
This allowed us to reduce the amount of nodes we had to expand significantly(9,551 vs 16,000 for UCS).
Though, it also isn't the tightest heuristic and can sometimes drastically underestimate the cost. 
For example, a very crowded environment with a bunch of dots in different directions all within a similar distance range to pacman would have a heuristic value that would be much less than the actual cost.

